{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "Tokenisation is the process of converting a stream of characters to tokens.\n",
    "Tokenisation can be implemented at two levels:\n",
    "- Sentence Level (sent_tokenize)\n",
    "- Word Level (word_tokenize)\n",
    "\n",
    "Important words:\n",
    "- Features: Features are the smallest unit in natural language. They are th individual words in a document\n",
    "- Document: A document is a single text datapoint. For example a tweet.\n",
    "- Lexicon: The complete vocabulary. Contains all the words and their meanings.\n",
    "- Corpora: A collection of documents. This is the complete universe of data. eg: entire wikipedia/ encyclopedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokeniser: ['hello mr. smith, how is your cookie ?', 'the weather is great, and the cookie is awesome.', 'the cookie is pinkish-blue.', \"you shouldn't eat so many cookie.\"]\n",
      "Word Tokeniser: ['hello', 'mr.', 'smith', ',', 'how', 'is', 'your', 'cookie', '?', 'the', 'weather', 'is', 'great', ',', 'and', 'the', 'cookie', 'is', 'awesome', '.', 'the', 'cookie', 'is', 'pinkish-blue', '.', 'you', 'should', \"n't\", 'eat', 'so', 'many', 'cookie', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello Mr. Smith, how is your cookie ? The weather is great, and the cookie is awesome. The cookie is pinkish-blue. You shouldn't eat so many cookie.\"\n",
    "text = text.lower()\n",
    "print(\"Sentence Tokeniser:\", sent_tokenize(text))\n",
    "print(\"Word Tokeniser:\", word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cookie', 'awesome', 'smith', 'pinkish-blue', 'eat', 'great', 'hello', 'weather', 'mr.', 'many'} 10\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stpwrds = stopwords.words()\n",
    "stpwrds.extend([\".\",\",\",\"?\",\"n't\"])\n",
    "features = set(word_tokenize(text)).difference(stpwrds)\n",
    "print(features, len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vectorisation\n",
    "\n",
    "Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.\n",
    "\n",
    "The process of converting words into numbers are called Vectorization\n",
    "Embeddings are the end product of vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Encoding\n",
    "\n",
    "- Frequency Based\n",
    "    - \n",
    "    - One hot Encoding\n",
    "    - Count Vectoriser\n",
    "    - Bag of Words\n",
    "\n",
    "- Prediction Based\n",
    "    - \n",
    "    - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello mr. smith, how is your cookie ?\n",
      "[0 0 1 0 0 1 0 1 0 1 0] 11\n",
      "the weather is great, and the cookie is awesome.\n",
      "[1 0 1 0 1 0 0 0 0 0 1] 11\n",
      "the cookie is pinkish-blue.\n",
      "[0 1 1 0 0 0 0 0 1 0 0] 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchaks/opt/anaconda3/envs/mis-382n/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'onların', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya', 'δι', 'арбаң', 'арсалаң', 'афташ', 'бай', 'бале', 'баски', 'батыр', 'баҳри', 'болои', 'бүгжең', 'бұтыр', 'валекин', 'вақте', 'вой', 'вуҷуди', 'гар', 'гарчанде', 'далаң', 'даме', 'ербелең', 'жалт', 'жұлт', 'карда', 'кошки', 'куя', 'күңгір', 'кӣ', 'магар', 'майлаш', 'митың', 'модоме', 'нияти', 'онан', 'оре', 'паһ', 'рӯи', 'салаң', 'сар', 'сұлаң', 'сұрт', 'тарбаң', 'тразе', 'ту', 'тыржың', 'тұрс', 'хом', 'хуб', 'чаро', 'чи', 'чун', 'чунон', 'шарте', 'шұңқ', 'ыржың', 'қадар', 'қайқаң', 'қалт', 'қаңғыр', 'қаңқ', 'қош', 'қызараң', 'құйқаң', 'құлт', 'құңқ', 'ұрс', 'ҳай', 'ҳамин', 'ҳатто', 'ҳо', 'ҳол', 'ҳолате', 'әттеген', 'ӯим', 'अक', 'अग', 'अझ', 'अन', 'अर', 'आजक', 'आत', 'आद', 'आफ', 'आय', 'ईक', 'उद', 'उनक', 'उनल', 'उह', 'एउट', 'एन', 'कog', 'कत', 'कम', 'कस', 'कसर', 'कह', 'गत', 'गय', 'गर', 'चम', 'छन', 'जत', 'जबक', 'जस', 'जसक', 'जसब', 'जसम', 'जसल', 'जह', 'तत', 'तथ', 'तदन', 'तप', 'तवम', 'नज', 'नत', 'नभन', 'नय', 'पक', 'पछ', 'पन', 'पय', 'पर', 'पष', 'पह', 'बन', 'बर', 'भएक', 'भय', 'भव', 'मल', 'यत', 'यथ', 'यद', 'यप', 'यसक', 'यसपछ', 'यसब', 'यसर', 'यह', 'रण', 'रत', 'रमश', 'रह', 'लस', 'वर', 'सक', 'सट', 'सध', 'सपछ', 'सब', 'सम', 'सर', 'सह', 'हन', 'हर', 'हरण', 'ἀλλ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectoriser = CountVectorizer(stop_words= stpwrds)\n",
    "X = count_vectoriser.fit_transform(sent_tokenize(text))\n",
    "\n",
    "for i in range(3):\n",
    "    print(sent_tokenize(text)[i])\n",
    "    print(X.toarray()[i], X.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello mr. smith, how is your cookie ?', 'the weather is great, and the cookie is awesome.', 'the cookie is pinkish-blue.', \"you shouldn't eat so many cookie.\"]\n",
      "['awesome', 'blue', 'cookie', 'eat', 'great', 'hello', 'many', 'mr', 'pinkish', 'smith', 'weather']\n",
      "    awesome      blue    cookie       eat     great     hello      many  \\\n",
      "0  0.000000  0.000000  0.288477  0.000000  0.000000  0.552805  0.000000   \n",
      "1  0.552805  0.000000  0.288477  0.000000  0.552805  0.000000  0.000000   \n",
      "2  0.000000  0.663385  0.346182  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.346182  0.663385  0.000000  0.000000  0.663385   \n",
      "\n",
      "         mr   pinkish     smith   weather  \n",
      "0  0.552805  0.000000  0.552805  0.000000  \n",
      "1  0.000000  0.000000  0.000000  0.552805  \n",
      "2  0.000000  0.663385  0.000000  0.000000  \n",
      "3  0.000000  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(sent_tokenize(text))\n",
    "tfifd_vectoriser = TfidfVectorizer(stop_words = stpwrds)\n",
    "# tfifd_vectoriser = TfidfVectorizer()\n",
    "tf_idf = tfifd_vectoriser.fit_transform(sent_tokenize(text))\n",
    "feature_names = tfifd_vectoriser.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "list_dense = pd.DataFrame(tf_idf.todense(), columns = feature_names)\n",
    "print(list_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine vs Euclidean Similarity\n",
    "\n",
    "Different methods to convert text to vectors:\n",
    "- Cosine Similarity\n",
    "    - \n",
    "- Euclidean Distance\n",
    "    - \n",
    "\n",
    "Difference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'NN'), ('mr.', 'NN'), ('smith', 'NNP'), (',', ','), ('how', 'WRB'), ('is', 'VBZ'), ('your', 'PRP$'), ('cookie', 'NN'), ('?', '.'), ('the', 'DT'), ('weather', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('cookie', 'NN'), ('is', 'VBZ'), ('awesome', 'JJ'), ('.', '.'), ('the', 'DT'), ('cookie', 'NN'), ('is', 'VBZ'), ('pinkish-blue', 'JJ'), ('.', '.'), ('you', 'PRP'), ('should', 'MD'), (\"n't\", 'RB'), ('eat', 'VB'), ('so', 'RB'), ('many', 'JJ'), ('cookie', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(nltk.pos_tag(word_tokenize(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30a65aae2e2006c4025eb4ed1e9e7ce326ed65d59eb79554b448ad831a6888a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('mis-382n': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
